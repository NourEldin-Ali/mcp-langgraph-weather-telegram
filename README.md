[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/noureldin-ali-mcp-langgraph-weather-telegram-badge.png)](https://mseep.ai/app/noureldin-ali-mcp-langgraph-weather-telegram)

# ğŸŒ¤ï¸ MCP + LangGraph: Weather â†’ Telegram (Streamlit)

Personal project that wires two MCP servers (Weather and Telegram) to LangGraph agents and simple Streamlit UIs. It fetches current weather and sends a concise summary to your Telegram chat. Includes both a fixed â€œFlow Agentâ€ and a toolâ€‘using â€œAgent Thinkâ€ mode.

<p align="center">
  <a href="https://www.python.org/"><img src="https://img.shields.io/badge/Python-3.11-3776AB?logo=python&logoColor=white" alt="Python"></a>
  <a href="https://streamlit.io/"><img src="https://img.shields.io/badge/Streamlit-FF4B4B?logo=streamlit&logoColor=white" alt="Streamlit"></a>
  <a href="https://langchain.com/"><img src="https://img.shields.io/badge/LangChain-1F6FEB?logo=chainlink&logoColor=white" alt="LangChain"></a>
  <a href="https://langchain-ai.github.io/langgraph/"><img src="https://img.shields.io/badge/LangGraph-1F6FEB" alt="LangGraph"></a>
  <a href="https://modelcontextprotocol.io/"><img src="https://img.shields.io/badge/MCP-000000" alt="MCP"></a>
  <a href="https://www.docker.com/"><img src="https://img.shields.io/badge/Docker-2496ED?logo=docker&logoColor=white" alt="Docker"></a>
  <a href="https://core.telegram.org/bots"><img src="https://img.shields.io/badge/Telegram-26A5E4?logo=telegram&logoColor=white" alt="Telegram"></a>
  <br/>
  <em>Tech stack at a glance</em>
  
</p>

## âœ¨ Highlights
- ğŸ›°ï¸ MCP servers over stdio:
  - â›… Weather: Openâ€‘Meteo geocoding + current weather (no API key)
  - âœˆï¸ Telegram: Bot API `sendMessage`
- ğŸ¤– Two agents and UIs:
  - ğŸ” Flow Agent: deterministic 3â€‘step graph (weather â†’ format â†’ send)
  - ğŸ§  Agent Think: toolâ€‘using ReActâ€‘style loop (multiâ€‘city, combined or separate messages)
- ğŸ§© Streamlit frontends; ğŸ³ Dockerfile + dockerâ€‘compose for oneâ€‘command run
- ğŸ”Œ Pluggable LLMs via `LLMConnector`: OpenAI, Groq, Azure OpenAI

## ğŸ” Flow Agent vs ğŸ§  Agent Think
- ğŸ¯ Purpose: Flow Agent runs a fixed pipeline; Agent Think plans tool calls.
- âŒ¨ï¸ Input style:
  - ğŸ” Flow Agent: one location input; always formats and sends one message.
  - ğŸ§  Agent Think: freeâ€‘form instruction; can handle multiple cities, combine results, or send separate messages.
- ğŸ§­ Control: Flow is simple and predictable. Agent Think decides when to call tools, how many times, and what to send.
- âœ… When to use:
  - Use ğŸ” Flow for fast singleâ€‘city updates to Telegram.
  - Use ğŸ§  Think for multiâ€‘city tasks or custom sending behavior.

### ğŸ§  Agent Think graph (example)

![Agent Think Graph](assets/agent-think-graph.png)

## ğŸš€ Quick Start (Local)
1) Create a virtualenv and install dependencies
```
python -m venv .venv
. .venv/Scripts/activate  # Windows
# source .venv/bin/activate  # macOS/Linux
pip install -r requirements.txt
```
2) Create and fill `.env` with your LLM provider key(s) and Telegram bot info. See â€œEnvironmentâ€ below. Do NOT commit real secrets to Git.

3) Run a Streamlit UI
```
# ğŸ” Flow Agent (single city â†’ one message)
streamlit run streamlit_app.py

# ğŸ§  Agent Think (freeâ€‘form, multiâ€‘city, combined/separate)
streamlit run streamlit_think_app.py
```
Open the printed URL and follow the prompts. By default (Docker compose): Flow â†’ http://localhost:8501 and Think â†’ http://localhost:8502.

## ğŸ³ Docker
Multiâ€‘stage Dockerfile exposes two targets and dockerâ€‘compose wires both:
```
docker compose up --build
# Flow UI â†’ http://localhost:8501
# Think UI â†’ http://localhost:8502
```
Build images separately (optional):
```
docker build -t mcp-weather-telegram:flow  --target flow  .
docker build -t mcp-weather-telegram:think --target think .
```

## ğŸ”‘ Environment
Set these in `.env` (example placeholders shown â€” keep real values private):
- LLM selection:
  - `LLM_TYPE` = `openai` | `groq` | `azure_openai`
  - `LLM_MODEL_NAME` (e.g., `gpt-4o-mini`, `gpt-4.1-mini`, `llama-3.1-70b-versatile`)
  - `LLM_TEMPERATURE` (default 0.2)
  - `LLM_MAX_RETRIES` (default 2)
- Provider keys:
  - `OPENAI_API_KEY` or `GROQ_API_KEY`
  - For Azure OpenAI: `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT`
- Telegram (for the Telegram MCP server):
  - `TELEGRAM_BOT_TOKEN`
  - `TELEGRAM_CHAT_ID` (user/group/channel)
- Weather default units:
  - `DEFAULT_WEATHER_UNITS` = `metric` | `imperial`

Tip: Commit an `.env.example` with placeholders, but never commit your real `.env`.

## âš™ï¸ How It Works
- MCP servers (stdio) are defined in `mcp_client/servers_config.json` and launched automatically:
  - `weather` â†’ `mcp_servers/weather_server/server.py`
  - `telegram` â†’ `mcp_servers/telegram_server/server.py`
- Flow Agent graph (`agent/graph.py`):
  - `get_weather` â†’ `format_message` (via LLM) â†’ `send_telegram`
- Agent Think (`agent_think/app.py`):
  - LLM is bound to tools (`get_weather`, `send_telegram`) and plans calls in a short loop.
  - Supports multiple cities and either one combined Telegram message or oneâ€‘perâ€‘city.

### ğŸ“¸ Optional Flow graph image

![Flow Agent Graph](assets/agent-flow-graph.png)

## ğŸ§ª Examples

### ğŸ” Flow Agent
- Input: enter a single location (e.g., â€œParis, FRâ€) and click Run.
- Output (Telegram message example):
![Flow UI Example](assets/ui-flow-example.png)
```
â€¢ Paris, FR â€” 18Â°C, feels 17Â°C
â€¢ Humidity 62% Â· Wind 8 km/h
```
Notes: Actual values depend on current weather and selected units.

### ğŸ§  Agent Think (combined)
- Instruction:
![Agent Think UI Example](assets/ui-think-example.png)
```
Get the weather for Paris and Berlin. Send a single concise Telegram message.
```
- Output (Telegram message example):
```
â€¢ Paris, FR: 18Â°C, clear Â· Wind 10 km/h
â€¢ Berlin, DE: 16Â°C, cloudy Â· Wind 12 km/h
```

### ğŸ§  Agent Think (separate)
- Instruction:
```
Get weather for Madrid and Barcelona and send separate Telegram messages for each.
```
- Output: two messages (one per city), for example:
```
â€¢ Madrid, ES: 24Â°C, sunny Â· Wind 9 km/h
â€¢ Barcelona, ES: 22Â°C, breezy Â· Wind 14 km/h
```
Notes: When unspecified, Agent Think defaults to a single combined message.

### ğŸ–¼ï¸ Screenshots
- Agent UI

![Agent UI](assets/agent-ui.png)

- Agent Results

![Agent Results](assets/agent-results.png)

## ğŸ“ Repository Layout (partial)
```
Dockerfile
docker-compose.yml
requirements.txt
streamlit_app.py                # Flow Agent UI
streamlit_think_app.py          # Agent Think UI
agent/                          # Flow Agent graph + nodes + state
agent_think/                    # Agent Think graph (tools + loop)
mcp_client/servers_config.json  # Where MCP servers are defined
mcp_servers/weather_server/     # Weather MCP server (Openâ€‘Meteo)
mcp_servers/telegram_server/    # Telegram MCP server (Bot API)
test/                           # Notebooks used during development
assets/                         # README images
```

## ğŸ“ Notes
- Your Telegram bot must have an open conversation with your account/group.
- `TELEGRAM_CHAT_ID` can be a user ID, group ID, or channel ID.
- No weather API key is required (Openâ€‘Meteo).
- In production, store secrets in a proper secret manager.

## âš ï¸ Disclaimer
This is a personal project for learning and demonstration purposes. It is not an official product and has no affiliation with OpenAI, Telegram, Openâ€‘Meteo, LangChain, or Streamlit. Use at your own risk and review the code and configuration before deploying anywhere sensitive.
## Telegram Listener Loop
If you want the agent to react to incoming Telegram chats without Streamlit, run the polling loop:

```
python -m agent_think.runner --timeout 20
```

The listener uses the same MCP servers configuration (`mcp_client/servers_config.json`). Ensure the following environment variables are set before starting it:
- `TELEGRAM_BOT_TOKEN`: Bot token used by the Telegram MCP server.
- `TELEGRAM_CHAT_ID`: Default chat for outbound messages (still required; per-message chats are derived from updates).
- LLM provider variables (`LLM_TYPE`, credentials) so the agent can plan tool calls.

The script keeps the latest `update_id` in memory while it is running; restart from scratch replays any unseen updates. Pass `--idle-sleep` to control how long the loop waits between empty polls, and adjust `--timeout` to tune long polling.
